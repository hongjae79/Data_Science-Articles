{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is a technique to find the input value that gives the mimium output of a differentiable function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let $J = w^2$.  Then find $w$ that gives the minimum $J$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./J.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, $J$ is a parabola and it has a global minimum $0$ at $w = 0$.  \n",
    "\n",
    "In calculus, we take the derivate, $\\frac{dJ}{dw} = 2w$.  And set it equal to $0$.  Then $2w = 0 $, therefore $w=0$.  We check the second derivative test to confirm $w_0$ gives the minumum value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in many real application problem, many problems may have lots of variables and computation might be very tedious and time consuming.  And even if we take the derivative and set the equation equal to $0$, there are many equations that are impossible to find the exact algebraic solution.  We can only approximate.   \n",
    "         \n",
    "The Gradient Descent is an alternate techinque that we can get away with many computational difficulties.  We put the process in recurrence sequence, so that the computer can perform the process in very short time and efficiently.  \n",
    "         \n",
    "We apply following the sequence:          \n",
    "$$w_{new} = w - \\alpha * \\frac{dJ}{dw}$$ \n",
    ",where $\\alpha$ is a learning rate. \n",
    "\n",
    "The question is then, where does this formula come from? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the function is increasing at $w_0$, we have the following picture. \n",
    "<img src=\"./gdinc.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is increasing at $w_0$.  So the minimum happens on the left of $w_0$.  This means we want to go to left direction.  That is $w_1$ is $w_0$ minus some positive number.  \n",
    "\n",
    "Want:  $w_1 = w_0 - $some positve number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the derivative at $w_0$, $\\frac{dJ}{dw}(w_0)$ is positive since the function at $w_0$ is increasing. Therefore, we have \n",
    "$$ w_1 = w_0 - \\alpha * \\frac{dJ}{dw}(w_0). $$  \n",
    "\n",
    ",where $\\alpha$ is a learning rate, a positve constant number so that we keep the subtracting positive number small enough to make the process efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decreaing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the function is decreasing, the we have the following picture. \n",
    "<img src=\"./gddec.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is decreasing at $w_0$, which means minimum happens at right side of $w_0$.  Therefore, we want to subtract some small negative number from $w_0$, so that we can go to right direction.  \n",
    "\n",
    "want: $w_1 = w_0 - $ some negatve number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the derivative at $w_0, \\frac{dJ}{dw}(w_0)$, is negative, so we have the following: $$w_1 = w_0 - \\alpha * \\frac{dJ}{dw}(w_0)$$\n",
    ",where $\\alpha$ is the learning rate again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether the function is increasing or decreasing the following formula: $$w_{new} = w - \\alpha * \\frac{dJ}{dw}(w)$$ \n",
    "\n",
    "We do this process many times, then we will have the $w$ which gives the minimum eventually.  I presented the two dimentional simple example above, but the gradient descent works in any dimensions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change the minus sign (-) to plus (+) then $$ w_{new} = w + \\alpha * \\frac{dJ}{dw}(w)$$, we are to find the $w$, which gives maximum.  And it is called gradient asecnt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with our example.  Let $w_0 = 1$, the initial value, and let $\\alpha = 0.1$.  $\\frac{dJ}{dw} = 2w$ by taking derivative.  Then\n",
    "\n",
    "$w_1 = w_0 - \\alpha * \\frac{dJ}{dw}(w_0) =  1 - 0.1 * 2(1) = 0.8$ \n",
    "\n",
    "$w_2 = 0.8 - 0.1 * 2 * 0.8 =  0.64$\n",
    "\n",
    "$w_3 = 0.64 - 0.1 * 2 * 0.64 = 0.512$ \n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$w_n \\approx 0$ \n",
    "\n",
    "We keep this process, then we get $w = 0 $ at the end.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recurrent process can be written in code quite simply.  \n",
    "\n",
    "```python\n",
    "w = 1\n",
    "for i in range(10):\n",
    "    w = w - 0.1 * 2 * w\n",
    "```\n",
    "Note that $\\alpha = 0.1$ and $\\frac{dJ}{dw} = 2w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code of the Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 1.000 at w0 = 1.000\n",
      "J = 0.640 at w1 = 0.800\n",
      "J = 0.410 at w2 = 0.640\n",
      "J = 0.262 at w3 = 0.512\n",
      "J = 0.168 at w4 = 0.410\n",
      "J = 0.107 at w5 = 0.328\n",
      "J = 0.069 at w6 = 0.262\n",
      "J = 0.044 at w7 = 0.210\n",
      "J = 0.028 at w8 = 0.168\n",
      "J = 0.018 at w9 = 0.134\n",
      "J = 0.012 at w10 = 0.107\n",
      "J = 0.007 at w11 = 0.086\n",
      "J = 0.005 at w12 = 0.069\n",
      "J = 0.003 at w13 = 0.055\n",
      "J = 0.002 at w14 = 0.044\n",
      "J = 0.001 at w15 = 0.035\n",
      "J = 0.001 at w16 = 0.028\n",
      "J = 0.001 at w17 = 0.023\n",
      "J = 0.000 at w18 = 0.018\n",
      "J = 0.000 at w19 = 0.014\n"
     ]
    }
   ],
   "source": [
    "w = 1 \n",
    "for i in range(20):\n",
    "    print(\"J = %.3f at w%d = %.3f\" % (w * w,i, w))\n",
    "    w = w - 0.1 * 2 * w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
